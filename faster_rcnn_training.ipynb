{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab09365d",
   "metadata": {
    "id": "vh54VEzbh_ky"
   },
   "source": [
    "## Installs and Imports\n",
    "Install and import PyTorch along with a few helper libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28714afd",
   "metadata": {},
   "source": [
    "## import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, r\"/home/sanjeev_kumar/work/polaris-ai-development-faster_rcnn_end_to_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5f349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e986cd46",
   "metadata": {
    "id": "bT_SgoOBh_kz"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# We will be reading images using OpenCV\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# matplotlib for visualization\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# basic python and ML Libraries\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pandas as pd\n",
    "import seaborn as sns\n",
    "# for ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# We will be reading images using OpenCV\n",
    "import cv2\n",
    "\n",
    "# matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# torchvision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# # helper libraries\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "# for image augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from collections import defaultdict\n",
    "from PIL import Image, ImageDraw,ImageFont \n",
    "import torchvision.transforms as transforms\n",
    "print(\"torch.cuda.is_available() : \",torch.cuda.is_available())\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ae3f4",
   "metadata": {
    "id": "SkwGcwwlh_k0"
   },
   "source": [
    "## The Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d809668",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCustomDataset : create class that \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mand return image , target dict \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomDataset\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    This is class create\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_dir, annot_path, transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CustomDataset : create class that \n",
    "and return image , target dict \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is class create\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, annot_path, transforms=False, normalize=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (path): image directory path\n",
    "            annot_path (json_path): coco json path\n",
    "            transforms (bool): if true, albumentation transfomation is applied on image\n",
    "            normalize (boll): if true , ToTensor transforation is applied on the image which convert image pixel to [0,1] and\n",
    "                                        return image in form (C,H, W)\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "        self.normalize = normalize\n",
    "        self.img_dir = img_dir\n",
    "        self.annot_path = annot_path\n",
    "        self.coco = COCO(self.annot_path)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "\n",
    "    #         print(\"self.img_ids : \",self.img_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_obj = self.coco.loadImgs(img_id)[0]\n",
    "        image_name = img_obj[\"file_name\"]\n",
    "        image_path = os.path.join(self.img_dir, image_name)\n",
    "        # image_path = image_path[:-4]+ image_path[-4:].upper() \n",
    "        image_path = image_path[:-4]+ image_path[-4:].lower() \n",
    "        # print(image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        anns_obj = self.coco.loadAnns(self.coco.getAnnIds(img_id))\n",
    "\n",
    "        anno_data = defaultdict(list)\n",
    "        for ann in anns_obj:\n",
    "            box = ann['bbox']\n",
    "            xmin = box[0]\n",
    "            ymin = box[1]\n",
    "            xmax = box[0] + box[2]\n",
    "            ymax = box[1] + box[3]\n",
    "            anno_data[\"boxes\"].append([xmin, ymin, xmax, ymax])\n",
    "            #             anno_data[\"boxes\"].append(ann['bbox'])\n",
    "            anno_data['area'].append(ann['area'])\n",
    "            anno_data['iscrowd'].append(ann['iscrowd'])\n",
    "            anno_data['label'].append(ann['category_id'])\n",
    "            anno_data[\"cat_name\"].append(self.get_cat_names(ann['category_id']))\n",
    "\n",
    "        # applying the albumenation transormation here for augmentation.\n",
    "        if self.transforms:\n",
    "            transformed_img, transformed_bboxes, transformed_class_labels = self.transform_img(img=image,\n",
    "                                                                                               bbox=anno_data['boxes'],\n",
    "                                                                                               label=anno_data[\"label\"])\n",
    "            # replcing the original image,box,label with transformed\n",
    "            image = transformed_img\n",
    "            anno_data['boxes'] = transformed_bboxes\n",
    "            anno_data[\"label\"] = transformed_class_labels\n",
    "\n",
    "        if self.normalize:  # converting image to normalize (pixel value 0 to 1) tensor (C,H,W) -> float32\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "            image = transform(image)\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.as_tensor(anno_data['boxes'], dtype=torch.int64),\n",
    "            'labels': torch.as_tensor(anno_data[\"label\"], dtype=torch.int64),\n",
    "            'image_id': torch.tensor([img_id]),  # pylint: disable=not-callable (false alarm)\n",
    "            'area': torch.as_tensor(anno_data['area'], dtype=torch.float32),\n",
    "            'iscrowd': torch.as_tensor(anno_data['iscrowd'], dtype=torch.int64),\n",
    "#             'cat_name': anno_data[\"cat_name\"],\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def transform_img(self, img, bbox, label):\n",
    "\n",
    "        \"\"\"\n",
    "        apply albumentation transoformation on image,bbox,label\n",
    "        Args:\n",
    "            self ():\n",
    "            img (numpy.ndarray): image to transform\n",
    "            bbox (list): bounding box list\n",
    "            label (list:str): label of each bounding box\n",
    "\n",
    "        Returns:\n",
    "            tuple of transformed_image,transformed_bboxes,transformed_labels\n",
    "        \"\"\"\n",
    "        # this is link alubumentation\n",
    "        # https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/    -- for bounding box understanding\n",
    "        # https://albumentations.ai/docs/ - doc file\n",
    "        # https://github.com/albumentations-team/albumentations  -- list of all other transform used in Albumentation\n",
    "\n",
    "        transform = A.Compose([\n",
    "            A.Resize(*img_size,),\n",
    "            A.HorizontalFlip(),\n",
    "            A.RandomBrightnessContrast(brightness_limit = 0.3, contrast_limit = 0.2), #p=0.3\n",
    "            # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.5, rotate_limit=55, p=0.3),\n",
    "            # A.SafeRotate(limit=45, p=1)\n",
    "        # ], bbox_params=A.BboxParams(format='pascal_voc', min_area=50, min_visibility=0.3, label_fields=['class_labels']))\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "        transformed = transform(image=img, bboxes=bbox, class_labels=label)\n",
    "        transformed_img = transformed['image']\n",
    "        transformed_bboxes = transformed['bboxes']\n",
    "        transformed_labels = transformed['class_labels']\n",
    "\n",
    "        if not transformed_bboxes:\n",
    "            transform = A.Compose([\n",
    "                                    A.Resize(*img_size,),\n",
    "                                    A.HorizontalFlip(p=0.5),\n",
    "                                    A.RandomBrightnessContrast(brightness_limit = 0.3, contrast_limit = 0.2, p=0.3), #p=0.3\n",
    "                                ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "            transformed = transform(image=img, bboxes=bbox, class_labels=label)\n",
    "            transformed_img = transformed['image']\n",
    "            transformed_bboxes = transformed['bboxes']\n",
    "            transformed_labels = transformed['class_labels']\n",
    "            return transformed_img, transformed_bboxes, transformed_labels\n",
    "\n",
    "        #     transformed_bboxes.append([0.0001, 0.0001, 0.0001, 0.0001])\n",
    "        #     transformed_labels.append(0)\n",
    "\n",
    "        return transformed_img, transformed_bboxes, transformed_labels\n",
    "\n",
    "    def get_cat_names(self, cat_ids=[]):\n",
    "        return [data['name'] for data in self.coco.loadCats(cat_ids)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67718d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebd0845c",
   "metadata": {
    "id": "4gktXqEih_k3"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Let's make some a helper function to view our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880d431d",
   "metadata": {
    "id": "WLZ-japJh_k4"
   },
   "outputs": [],
   "source": [
    "def draw_bounding_box(img, pred_dict={},text_color=\"red\",text_font_size = 50,bbox_color = \"white\",bbox_thickness = 5):  # drow bounding box on the image\n",
    "    id_2_class = {0: '_background_', 1: 'tire_marks', 2: 'trackout', 3: 'unclear'}\n",
    "    if isinstance(img,np.ndarray):\n",
    "        image =Image.fromarray(img.copy())\n",
    "    else:\n",
    "        image =img.copy()\n",
    "    font = ImageFont.truetype(r\"/home/sanjeev_kumar/work/trackouts/SiemensSans_Global_Roman.ttf\", size=text_font_size)\n",
    "    bbox_list = pred_dict[\"boxes\"].cuda().cpu().numpy()\n",
    "    labels = pred_dict[\"labels\"].cuda().cpu().numpy()\n",
    "    scores = pred_dict[\"scores\"].cuda().cpu().numpy()\n",
    "    for i,bbox in enumerate(bbox_list):\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        start_point = (xmin, ymax)\n",
    "        end_point = (xmax, ymin)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.rectangle((start_point, end_point), outline =bbox_color,width=bbox_thickness)\n",
    "        text_point =(xmin, ymin - 50)\n",
    "        draw.text(text_point, id_2_class[labels[i]] , fill=text_color,font=font)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to visualize bounding boxes in the image\n",
    "def plot_img_bbox(img, target):\n",
    "  # plot the image and bboxes\n",
    "  # Bounding boxes are defined as follows: x-min y-min width height\n",
    "#     img = torch.tensor(img.numpy()*255,dtype=torch.uint8)\n",
    "    if isinstance(img,torch.Tensor):\n",
    "        img = torchtrans.ToPILImage()(img).convert('RGB')\n",
    "        \n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(5,5)\n",
    "    a.imshow(img)\n",
    "    boxes  = target['boxes']\n",
    "    if isinstance(boxes,torch.Tensor):\n",
    "        boxes = boxes.cpu()\n",
    "    for box in (boxes):\n",
    "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "        rect = patches.Rectangle(\n",
    "          (x, y),\n",
    "          width, height,\n",
    "          linewidth = 2,\n",
    "          edgecolor = 'r',\n",
    "          facecolor = 'none'\n",
    "        )\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83a01b",
   "metadata": {},
   "source": [
    "## collate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acfc999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    batch = filter (lambda x:x is not None, batch)\n",
    "    return utils.collate_fn(batch)\n",
    "\n",
    "\n",
    "# def my_collate(batch, dataset):\n",
    "#     len_batch = len(batch)\n",
    "#     batch = list(filter(lambda x: x is not None, batch))\n",
    "\n",
    "#     if len_batch > len(batch):                \n",
    "#         db_len = len(dataset)\n",
    "#         diff = len_batch - len(batch)\n",
    "#         while diff != 0:\n",
    "#             a = dataset[np.random.randint(0, db_len)]\n",
    "#             if a is None:                \n",
    "#                 continue\n",
    "#             batch.append(a)# Dataloaders\n",
    "\n",
    "# Make a loader for feeding our data into the neural network\n",
    "#             diff -= 1\n",
    "\n",
    "#     return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bb0c9",
   "metadata": {
    "id": "cmAdf8Yih_k5"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Make a loader for feeding our data into the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828a544",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747a780c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# train on gpu if available\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "img_size = [1024,1024]\n",
    "# img_size = [512,512] #[1024,1024]\n",
    "num_worker = 0\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "# train on gpu if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 4 # one class (class 0) is dedicated to the \"background\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e78b3",
   "metadata": {},
   "source": [
    "### working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7fbe486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(\"before : \",os.getcwd())\n",
    "# os.chdir(r\"/home/sanjeev_kumar/work\")\n",
    "# print(\"after : \",os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7ca4b",
   "metadata": {},
   "source": [
    "### path and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "655a8306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\work\\\\downloaded_zip\\\\temp'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66ff77d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"trackouts_patch/Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca820fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m VALID_ANNOT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musecase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# TRAIN_ANNOT_PATH = r\"/home/sanjeev_kumar/work/trakouts/train.json\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# VALID_ANNOT_PATH = r\"/home/sanjeev_kumar/work/trakouts/test.json\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print('Image shape:', img.shape)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print('Label example:', target)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m(IMG_DIR,VALID_ANNOT_PATH,transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m img, target \u001b[38;5;241m=\u001b[39m test_dataset[\u001b[38;5;241m8\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
     ]
    }
   ],
   "source": [
    "usecase_name = \"trackouts_patch\" #coco_fridge_data  fpc\n",
    "IMG_DIR = f'./{usecase_name}/Images/'\n",
    "TRAIN_ANNOT_PATH = f\"./{usecase_name}/train.json\"\n",
    "# validation images and test.json files directory/path\n",
    "VALID_ANNOT_PATH = f\"./{usecase_name}/test.json\"\n",
    "\n",
    "# TRAIN_ANNOT_PATH = r\"/home/sanjeev_kumar/work/trakouts/train.json\"\n",
    "# VALID_ANNOT_PATH = r\"/home/sanjeev_kumar/work/trakouts/test.json\"\n",
    "\n",
    "# train_dataset = CustomDataset(IMG_DIR,TRAIN_ANNOT_PATH,transforms=True, normalize=True)\n",
    "# img, target = train_dataset[6]\n",
    "# print('Image shape:', img.shape)\n",
    "# print('Label example:', target)\n",
    "test_dataset = CustomDataset(IMG_DIR,VALID_ANNOT_PATH,transforms=True, normalize=True)\n",
    "img, target = test_dataset[8]\n",
    "print('Image shape:', img.shape)\n",
    "print('Label example:', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f921848",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# plotting the image with bboxes. Feel free to change the index\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      3\u001b[0m plot_img_bbox(img, target)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# plotting the image with bboxes. Feel free to change the index\n",
    "img, target = test_dataset[3]\n",
    "plot_img_bbox(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5d83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09460a07",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63946c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # use our dataset and defined transformations\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m(IMG_DIR,TRAIN_ANNOT_PATH,transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m  CustomDataset(IMG_DIR,VALID_ANNOT_PATH,transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_collate\u001b[39m(batch):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# # use our dataset and defined transformations\n",
    "\n",
    "dataset = CustomDataset(IMG_DIR,TRAIN_ANNOT_PATH,transforms=True, normalize=True)\n",
    "dataset_test =  CustomDataset(IMG_DIR,VALID_ANNOT_PATH,transforms=True, normalize=True)\n",
    "\n",
    "def my_collate(batch):\n",
    "    batch = filter (lambda x:x is not None, batch)\n",
    "    return utils.collate_fn(batch)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "  dataset,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True,\n",
    "  num_workers=num_worker,\n",
    "  # collate_fn=my_collate,\n",
    "    collate_fn = utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "  dataset_test,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  num_workers=num_worker,\n",
    "  # collate_fn=my_collate,\n",
    "  collate_fn =utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4841d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664263b",
   "metadata": {
    "id": "7HpLH-obh_k4"
   },
   "source": [
    "## Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9912f491",
   "metadata": {
    "id": "PLGLZVbRh_k4"
   },
   "outputs": [],
   "source": [
    "def get_object_detection_model(num_classes,pred=False):\n",
    "    # load a model pre-trained pre-trained on COCO\n",
    "    if pred:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "    else:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bfde3d",
   "metadata": {},
   "source": [
    "## eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "712a4885",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 107>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m losses, detections\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Tuple\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat_box_prediction_layers\u001b[39m(box_cls: List[Tensor], box_regression: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m    110\u001b[0m     box_cls_flattened \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def eval_forward(model, images, targets):\n",
    "    \"\"\"type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images (list[Tensor]): images to be processed\n",
    "        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "    Returns:\n",
    "        result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "            It returns list[BoxList] contains additional fields\n",
    "            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    original_image_sizes: List[Tuple[int, int]] = []\n",
    "    for img in images:\n",
    "        val = img.shape[-2:]\n",
    "        assert len(val) == 2\n",
    "        original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "    images, targets = model.transform(images, targets)\n",
    "\n",
    "    # Check for degenerate boxes\n",
    "    # TODO: Move this to a function\n",
    "    if targets is not None:\n",
    "        for target_idx, target in enumerate(targets):\n",
    "            boxes = target[\"boxes\"]\n",
    "            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "            if degenerate_boxes.any():\n",
    "                # print the first degenerate box\n",
    "                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                raise ValueError(\n",
    "                    \"All bounding boxes should have positive height and width.\"\n",
    "                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
    "                )\n",
    "\n",
    "    features = model.backbone(images.tensors)\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = OrderedDict([(\"0\", features)])\n",
    "    model.rpn.training=True\n",
    "    #model.roi_heads.training=True\n",
    "\n",
    "\n",
    "    #####proposals, proposal_losses = model.rpn(images, features, targets)\n",
    "    features_rpn = list(features.values())\n",
    "    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
    "    anchors = model.rpn.anchor_generator(images, features_rpn)\n",
    "\n",
    "    num_images = len(anchors)\n",
    "    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "    # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "    # the proposals\n",
    "    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "    proposals = proposals.view(num_images, -1, 4)\n",
    "    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "    proposal_losses = {}\n",
    "    assert targets is not None\n",
    "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
    "    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
    "    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
    "        objectness, pred_bbox_deltas, labels, regression_targets\n",
    "    )\n",
    "    proposal_losses = {\n",
    "        \"loss_objectness\": loss_objectness,\n",
    "        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "    }\n",
    "\n",
    "    #####detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "    image_shapes = images.image_sizes\n",
    "    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n",
    "    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
    "    box_features = model.roi_heads.box_head(box_features)\n",
    "    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
    "\n",
    "    result: List[Dict[str, torch.Tensor]] = []\n",
    "    detector_losses = {}\n",
    "    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
    "    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
    "    boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
    "    num_images = len(boxes)\n",
    "    for i in range(num_images):\n",
    "        result.append(\n",
    "            {\n",
    "                \"boxes\": boxes[i],\n",
    "                \"labels\": labels[i],\n",
    "                \"scores\": scores[i],\n",
    "            }\n",
    "        )\n",
    "    detections = result\n",
    "    detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "    model.rpn.training=False\n",
    "    model.roi_heads.training=False\n",
    "    losses = {}\n",
    "    losses.update(detector_losses)\n",
    "    losses.update(proposal_losses)\n",
    "    return losses, detections\n",
    "\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from torch import nn, Tensor\n",
    "\n",
    "def concat_box_prediction_layers(box_cls: List[Tensor], box_regression: List[Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "    box_cls_flattened = []\n",
    "    box_regression_flattened = []\n",
    "    # for each feature level, permute the outputs to make them be in the\n",
    "    # same format as the labels. Note that the labels are computed for\n",
    "    # all feature levels concatenated, so we keep the same representation\n",
    "    # for the objectness and the box_regression\n",
    "    for box_cls_per_level, box_regression_per_level in zip(box_cls, box_regression):\n",
    "        N, AxC, H, W = box_cls_per_level.shape\n",
    "        Ax4 = box_regression_per_level.shape[1]\n",
    "        A = Ax4 // 4\n",
    "        C = AxC // A\n",
    "        box_cls_per_level = permute_and_flatten(box_cls_per_level, N, A, C, H, W)\n",
    "        box_cls_flattened.append(box_cls_per_level)\n",
    "\n",
    "        box_regression_per_level = permute_and_flatten(box_regression_per_level, N, A, 4, H, W)\n",
    "        box_regression_flattened.append(box_regression_per_level)\n",
    "    # concatenate on the first dimension (representing the feature levels), to\n",
    "    # take into account the way the labels were generated (with all feature maps\n",
    "    # being concatenated as well)\n",
    "    box_cls = torch.cat(box_cls_flattened, dim=1).flatten(0, -2)\n",
    "    box_regression = torch.cat(box_regression_flattened, dim=1).reshape(-1, 4)\n",
    "    return box_cls, box_regression\n",
    "\n",
    "def permute_and_flatten(layer: Tensor, N: int, A: int, C: int, H: int, W: int) -> Tensor:\n",
    "    layer = layer.view(N, -1, C, H, W)\n",
    "    layer = layer.permute(0, 3, 4, 1, 2)\n",
    "    layer = layer.reshape(N, -1, C)\n",
    "    return layer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n",
    "    # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n",
    "    \"\"\"\n",
    "    Computes the loss for Faster R-CNN.\n",
    "    Args:\n",
    "        class_logits (Tensor)\n",
    "        box_regression (Tensor)\n",
    "        labels (list[BoxList])\n",
    "        regression_targets (Tensor)\n",
    "    Returns:\n",
    "        classification_loss (Tensor)\n",
    "        box_loss (Tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    regression_targets = torch.cat(regression_targets, dim=0)\n",
    "\n",
    "    classification_loss = F.cross_entropy(class_logits, labels)\n",
    "\n",
    "    # get indices that correspond to the regression targets for\n",
    "    # the corresponding ground truth labels, to be used with\n",
    "    # advanced indexing\n",
    "    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n",
    "    labels_pos = labels[sampled_pos_inds_subset]\n",
    "    N, num_classes = class_logits.shape\n",
    "    box_regression = box_regression.reshape(N, box_regression.size(-1) // 4, 4)\n",
    "\n",
    "    box_loss = F.smooth_l1_loss(\n",
    "        box_regression[sampled_pos_inds_subset, labels_pos],\n",
    "        regression_targets[sampled_pos_inds_subset],\n",
    "        beta=1 / 9,\n",
    "        reduction=\"sum\",\n",
    "    )\n",
    "    box_loss = box_loss / labels.numel()\n",
    "\n",
    "    return classification_loss, box_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42ba6c",
   "metadata": {},
   "source": [
    "## evaluate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb0d8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(model, data_loader, device):\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for images, targets in data_loader:\n",
    "          images = list(image.to(device) for image in images)\n",
    "          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "          losses_dict, detections = eval_forward(model, images, targets)\n",
    "          losses = sum(loss for loss in losses_dict.values())\n",
    "          \n",
    "          val_loss += losses\n",
    "          \n",
    "    validation_loss = val_loss/ len(data_loader)    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0ed28",
   "metadata": {},
   "source": [
    "## apply_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b883828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes the original prediction and the iou threshold.\n",
    "def apply_nms(orig_prediction, iou_thresh=0.5):\n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "\n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "\n",
    "    return final_prediction\n",
    "\n",
    "# function to convert a torchtensor back to PIL image\n",
    "def torch_to_pil(img):\n",
    "    return torchtrans.ToPILImage()(img).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69057854",
   "metadata": {},
   "source": [
    "## EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02d0ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            print(\"Metric reached nan, stopping training!!\")\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print(\"Training stopped since metric did not improve!!\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5ed28",
   "metadata": {
    "id": "OPsWQFoah_k5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d27e0",
   "metadata": {
    "id": "u-0h9b1Th_k5"
   },
   "source": [
    "Let's prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94355fb",
   "metadata": {
    "id": "qP7T0cA-h_k5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes : \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mnum_classes\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# get the model using our helper function\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m get_object_detection_model(num_classes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"num_classes : \",num_classes)\n",
    "# get the model using our helper function\n",
    "model = get_object_detection_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.09, weight_decay=0.0005)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.005, weight_decay=0.0005)\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "  optimizer,\n",
    "  step_size=3,\n",
    "  gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0d9fccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for epochs =  100\n",
      "batch_size :  8\n"
     ]
    }
   ],
   "source": [
    "print(\"training for epochs = \",num_epochs)\n",
    "print(\"batch_size : \",batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f1daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "795371d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_one_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m mAP_score_list \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 8\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_one_epoch\u001b[49m(model, optimizer, data_loader, device, epoch, print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_one_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(patience=50)\n",
    "\n",
    "train_loss_list =[]\n",
    "val_loss_list =[]\n",
    "mAP_score_list =[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        train_loss =  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        train_loss_list.append(train_loss)\n",
    "        print('train_loss_list',train_loss_list)\n",
    "        # mAP_score_list.append(mAP_score)\n",
    "        # print('map score list',mAP_score_list)\n",
    "        lr_scheduler.step()\n",
    "        validation_loss  = evaluate_loss(model, data_loader_test, device=device)\n",
    "        print(f\"Validation loss ------------------> {validation_loss}\")\n",
    "        val_loss_list.append(validation_loss.cpu().item())\n",
    "        # writer.add_scalar('validation loss',validation_loss,epoch)\n",
    "        if es.step(torch.tensor([validation_loss])):\n",
    "                break\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        mAP = evaluate(model, data_loader_test, device=device)\n",
    "        mAP_score_list.append(mAP)\n",
    "        print('map score list',mAP_score_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46f1d7a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_one_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m mAP_score_list \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 8\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_one_epoch\u001b[49m(model, optimizer, data_loader, device, epoch, print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_one_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(patience=50)\n",
    "\n",
    "train_loss_list =[]\n",
    "val_loss_list =[]\n",
    "mAP_score_list =[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        train_loss =  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        train_loss_list.append(train_loss)\n",
    "        print('train_loss_list',train_loss_list)\n",
    "        # mAP_score_list.append(mAP_score)\n",
    "        # print('map score list',mAP_score_list)\n",
    "        lr_scheduler.step()\n",
    "        validation_loss  = evaluate_loss(model, data_loader_test, device=device)\n",
    "        print(f\"Validation loss ------------------> {validation_loss}\")\n",
    "        val_loss_list.append(validation_loss.cpu().item())\n",
    "        # writer.add_scalar('validation loss',validation_loss,epoch)\n",
    "        if es.step(torch.tensor([validation_loss])):\n",
    "                break\n",
    "        \n",
    "        # evaluate on the test dataset\n",
    "        mAP = evaluate(model, data_loader_test, device=device)\n",
    "        mAP_score_list.append(mAP)\n",
    "        print('map score list',mAP_score_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c8ce6ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'trackouts_patch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# mAP_df = pd.DataFrame(data=data1)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# mAP_df.to_csv('mAP/exp2.csv')\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mloss_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43musecase_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/faster_rcnn_loss_1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3542\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3554\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3556\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:697\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 697\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:571\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'trackouts_patch'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m loss_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# mAP_df = pd.DataFrame(data=data1)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# mAP_df.to_csv('mAP/exp2.csv'2\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43musecase_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/faster_rcnn_loss_1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3554\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3556\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:697\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 697\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:571\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    569\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'trackouts_patch'"
     ]
    }
   ],
   "source": [
    "\n",
    "try: \n",
    "    data = {'train_loss': train_loss_list,'val_loss':val_loss_list, 'mAP':mAP_score_list}\n",
    "    # data1 = {'mAP':mAP_score_list}\n",
    "    loss_df = pd.DataFrame(data=data)\n",
    "    # mAP_df = pd.DataFrame(data=data1)\n",
    "    # mAP_df.to_csv('mAP/exp2.csv')\n",
    "    loss_df.to_csv(f'./{usecase_name}/faster_rcnn_loss_1.csv')\n",
    "except:\n",
    "    data = {'train_loss': train_loss_list[:-1],'val_loss':val_loss_list[:-1], 'mAP':mAP_score_list}\n",
    "    # data1 = {'mAP':mAP_score_list}\n",
    "    loss_df = pd.DataFrame(data=data)\n",
    "    # mAP_df = pd.DataFrame(data=data1)\n",
    "    # mAP_df.to_csv('mAP/exp2.csv'2\n",
    "    loss_df.to_csv(f'./{usecase_name}/faster_rcnn_loss_1.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc803f9f",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20046686",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_weight = f\"./{usecase_name}/fasterrcnn_\"+str(num_epochs)+str(\"_\")+f\"{datetime.datetime.now():%Y_%m_%d}\"+\".pt\"\n",
    "model_name_shape = f\"./{usecase_name}/fasterrcnn_shape_\"+str(num_epochs)+str(\"_\")+f\"{datetime.datetime.now():%Y_%m_%d}\"+\".pt\"\n",
    "print(\"model_name_weight : \", model_name_weight)\n",
    "print(\"model_name_shape : \", model_name_shape)\n",
    "torch.save(model.state_dict(),model_name_weight)\n",
    "torch.save(model, model_name_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c2de0",
   "metadata": {},
   "source": [
    "## training  plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = sns.lineplot(data = loss_df)\n",
    "\n",
    "plt.ylim(top=1) #ymax is your value\n",
    "plt.ylim(bottom=0) #ymin is your value\n",
    "plt.savefig( f'./{usecase_name}/loss_demo2.png')\n",
    "plt.show(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d654ae",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b66c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('fasterrcnn_shape.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f53725",
   "metadata": {},
   "source": [
    "# Model prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "040be5eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# pick one image from the test set\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_test\u001b[49m[idx]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(img))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_test' is not defined"
     ]
    }
   ],
   "source": [
    "# test_dataset = ConeImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n",
    "idx=1\n",
    "# pick one image from the test set\n",
    "img, target = dataset_test[idx]\n",
    "print(img.shape)\n",
    "print(type(img))\n",
    "# i =img\n",
    "# put the model in evaluation mode\n",
    "# model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])[0]\n",
    "# print(\"type of prediction : \",type(prediction))\n",
    "print(\"prediction : \",prediction)\n",
    "print('MODEL OUTPUT\\n')\n",
    "nms_prediction = apply_nms(prediction, iou_thresh=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67704222",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m torch_to_pil(\u001b[43mimg\u001b[49m)\n\u001b[0;32m      2\u001b[0m bbox_img \u001b[38;5;241m=\u001b[39m draw_bounding_box(img,nms_prediction)\n\u001b[0;32m      3\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "img = torch_to_pil(img)\n",
    "bbox_img = draw_bounding_box(img,nms_prediction)\n",
    "save_dir =r\"/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/\"\n",
    "save_path = save_dir +\"pred_\"+str(idx)+\".JPG\"\n",
    "bbox_img.save(save_path)\n",
    "\n",
    "# bbox_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcb92c88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m      3\u001b[0m     idx\u001b[38;5;241m=\u001b[39mi\n\u001b[1;32m----> 4\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_test\u001b[49m[idx]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# model.cuda()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_test' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    \n",
    "    idx=i\n",
    "    img, target = dataset_test[idx]\n",
    "    # model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])[0]\n",
    "    nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n",
    "\n",
    "    img = torch_to_pil(img)\n",
    "    bbox_img = draw_bounding_box(img,nms_prediction)\n",
    "    save_dir =r\"/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/\"\n",
    "    save_path = save_dir +\"pred_\"+str(idx)+\".JPG\"\n",
    "    bbox_img.save(save_path)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1214b0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/*\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(glob.glob(\"/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9718662e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m torch_to_pil(\u001b[43mimg\u001b[49m)\n\u001b[0;32m      2\u001b[0m bbox_img \u001b[38;5;241m=\u001b[39m draw_bounding_box(img,nms_prediction)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# bbox_img.save(IMG_SAVE_PATH+name+ext\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "img = torch_to_pil(img)\n",
    "bbox_img = draw_bounding_box(img,nms_prediction)\n",
    "# bbox_img.save(IMG_SAVE_PATH+name+ext\n",
    "bbox_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12b45afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# 'train': transforms.Compose([\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#     transforms.Resize((input_image_height, input_image_width)),\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#     transforms.ToTensor(),\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#     # transforms.Normalize(model_mean, model_std)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# ]),\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      8\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize(img_size[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m      9\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# transforms.Normalize(model_mean, model_std)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     ]),\n\u001b[0;32m     12\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    # 'train': transforms.Compose([\n",
    "    #     transforms.Resize((input_image_height, input_image_width)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # transforms.Normalize(model_mean, model_std)\n",
    "    # ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(img_size[0]),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(model_mean, model_std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c713106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sanjeev_kumar/work/trackouts/Images'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/home/sanjeev_kumar/work/trackouts/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ac86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c589e6ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sanjeev_kumar/work/usecase/trackouts_patch/new_blind_img/DJI_20220928155042_0007_crop.JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# img_path = \"./trackouts/Images/DJI_20211123144914_0002_W.jpg\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sanjeev_kumar/work/usecase/trackouts_patch/new_blind_img/DJI_20220928155042_0007_crop.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m png_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m png_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(png_image)\n\u001b[0;32m      9\u001b[0m png_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(png_array)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 2953\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2954\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sanjeev_kumar/work/usecase/trackouts_patch/new_blind_img/DJI_20220928155042_0007_crop.JPG'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import sys\n",
    "# img_path = \"./trackouts/Images/DJI_20211123144914_0002_W.jpg\"\n",
    "img_path = \"/home/sanjeev_kumar/work/usecase/trackouts_patch/new_blind_img/DJI_20220928155042_0007_crop.JPG\"\n",
    "\n",
    "\n",
    "png_image = Image.open(img_path)\n",
    "png_array = np.array(png_image)\n",
    "png_image = Image.fromarray(png_array)\n",
    "png_image = data_transforms['val'](png_image)\n",
    "png_image = png_image.to(torch.float)\n",
    "# png_image = torch.unsqueeze(png_image, 0)\n",
    "png_image = png_image.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([png_image])[0]\n",
    "    \n",
    "print('MODEL OUTPUT\\n')\n",
    "nms_prediction = apply_nms(prediction, iou_thresh=0.00)\n",
    "plot_img_bbox(torch_to_pil(img), nms_prediction)\n",
    "plot_img_bbox(png_image, nms_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch_to_pil(img.cuda().cpu())\n",
    "bbox_img = draw_bounding_box(img,nms_prediction)\n",
    "# bbox_img.save(IMG_SAVE_PATH+name+ext)\n",
    "bbox_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23a69d",
   "metadata": {},
   "source": [
    "# Sahi prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6c310ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sahi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msahi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_sliced_prediction, predict, get_prediction\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msahi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoDetectionModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sahi'"
     ]
    }
   ],
   "source": [
    "from sahi.predict import get_sliced_prediction, predict, get_prediction\n",
    "from sahi import AutoDetectionModel\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02c8d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sahi_pred(result_dict,image_size = (1024, 1024)):\n",
    "    # print(\"len of the result : \",len(result_dict))\n",
    "    # bbox_list,score_list,label_list,cat_name_list =[],[],[],[]\n",
    "    sahi_pred_list=[]\n",
    "    if len(result_dict)==1:\n",
    "        temp={}\n",
    "        # try:\n",
    "        #     l = len(result_dict[0]['bbox'][0])\n",
    "        #     islist= True\n",
    "            \n",
    "        # except :\n",
    "        #     islist = False\n",
    "        # if islist:\n",
    "        #     temp[\"boxes\"] =result_dict[0]['bbox']\n",
    "        #     temp[\"labels\"] =result_dict[0]['category_id']\n",
    "        #     temp[\"scores\"] =result_dict[0]['score']\n",
    "              \n",
    "        # else:\n",
    "        #     temp[\"boxes\"] =[result_dict[0]['bbox']]\n",
    "        #     temp[\"labels\"] = [result_dict[0]['category_id']]\n",
    "        #     temp[\"scores\"] =[result_dict[0]['score']]\n",
    "\n",
    "        temp[\"boxes\"] =[result_dict[0]['bbox']]\n",
    "        temp[\"labels\"] = [result_dict[0]['category_id']]\n",
    "        temp[\"scores\"] =[result_dict[0]['score']]\n",
    "        sahi_pred_list.append(temp)\n",
    "        return sahi_pred_list\n",
    "\n",
    "\n",
    "    for item in result_dict:\n",
    "        temp={}\n",
    "        temp[\"boxes\"] =item['bbox']\n",
    "        temp[\"labels\"] =item['category_id']\n",
    "        temp[\"scores\"] =item['score']\n",
    "        sahi_pred_list.append(temp)\n",
    "\n",
    "\n",
    "    return sahi_pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224390ea",
   "metadata": {},
   "source": [
    "# unseen data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "068d4193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_path_list :  0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m img_path_list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(UNSEEN_DATA_PATH\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.JPG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_path_list : \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(img_path_list))\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimg_path_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# /home/sanjeev_kumar/work/usecase/trackouts_patch/new_unseen_data/DJI_20221020112922_0049.JPG\n",
    "UNSEEN_DATA_PATH =r\"/home/sanjeev_kumar/work/usecase/trackouts_patch/new_blind_img/\"\n",
    "import glob\n",
    "img_path_list = glob.glob(UNSEEN_DATA_PATH+'*.JPG')\n",
    "print(\"img_path_list : \",len(img_path_list))\n",
    "print(img_path_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf714520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sahi_draw_bounding_box(img_path, pred_list=[],text_color=\"red\",text_font_size = 50,bbox_color = \"white\",bbox_thickness = 5):  # drow bounding box on the image\n",
    "    id_2_class = {0: '_background_', 1: 'tire_marks', 2: 'trackout', 3: 'unclear'}\n",
    "    image = Image.open(img_path)\n",
    "    font = ImageFont.truetype(r\"/home/sanjeev_kumar/work/trackouts/SiemensSans_Global_Roman.ttf\", size=text_font_size)\n",
    "    for pred_dict in pred_list:\n",
    "        bbox_list = pred_dict[\"boxes\"]\n",
    "        labels = pred_dict[\"labels\"]\n",
    "        scores = pred_dict[\"scores\"]\n",
    "\n",
    "        # xmin, ymin, xmax, ymax = bbox\n",
    "        xmin, ymin, xmax, ymax = pred_dict[\"boxes\"]\n",
    "        xmin, ymin, xmax, ymax = xmin, ymin, xmin+xmax, ymin+ymax\n",
    "\n",
    "        start_point = (xmin, ymax)\n",
    "        end_point = (xmax, ymin)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.rectangle((start_point, end_point), outline =bbox_color,width=bbox_thickness)\n",
    "        text_point =(xmin, ymin - 50)\n",
    "        draw.text(text_point, id_2_class[labels] , fill=text_color,font=font)\n",
    "\n",
    "        # for i,bbox in enumerate(bbox_list):\n",
    "        #     # xmin, ymin, xmax, ymax = bbox\n",
    "        #     xmin, ymin, xmax, ymax = bbox\n",
    "        #     xmin, ymin, xmax, ymax = xmin, ymin, xmin+xmax, ymin+ymax\n",
    "\n",
    "        #     start_point = (xmin, ymax)\n",
    "        #     end_point = (xmax, ymin)\n",
    "        #     draw = ImageDraw.Draw(image)\n",
    "        #     draw.rectangle((start_point, end_point), outline =bbox_color,width=bbox_thickness)\n",
    "        #     text_point =(xmin, ymin - 50)\n",
    "        #     draw.text(text_point, id_2_class[labels[i]] , fill=text_color,font=font)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfad6427",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5cdf5be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoDetectionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sahi_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoDetectionModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      2\u001b[0m             model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m             confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m      5\u001b[0m             image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      6\u001b[0m             device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# or \"cuda:0\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m             load_at_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m             category_mapping \u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_background_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtire_marks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrackout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munclear\u001b[39m\u001b[38;5;124m'\u001b[39m} )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoDetectionModel' is not defined"
     ]
    }
   ],
   "source": [
    "sahi_model = AutoDetectionModel.from_pretrained(\n",
    "            model_type='torchvision',\n",
    "            model=model,\n",
    "            confidence_threshold=0.5,\n",
    "            image_size=1024,\n",
    "            device=\"cpu\", # or \"cuda:0\"\n",
    "            load_at_init=True,\n",
    "            category_mapping ={\"0\": '_background_', \"1\": 'tire_marks', \"2\": 'trackout', \"3\": 'unclear'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "315cc334",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_sliced_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m sahi_img_path \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sanjeev_kumar/work/trackouts/Images/DJI_20211123144914_0002_W.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# sahi_img_path= img_path_list[idx]\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mget_sliced_prediction\u001b[49m(\n\u001b[0;32m      8\u001b[0m     sahi_img_path,\n\u001b[0;32m      9\u001b[0m     sahi_model,\n\u001b[0;32m     10\u001b[0m     slice_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m     11\u001b[0m     slice_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m     12\u001b[0m     overlap_height_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     13\u001b[0m     overlap_width_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m     14\u001b[0m     postprocess_match_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m  \n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_sliced_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "idx=1 # till 25\n",
    "# sahi_img_path = r\"/home/sanjeev_kumar/work/usecase/trackouts_patch/Images/DJI_20220928175758_0125_2460_820_3484_1844.jpg\"\n",
    "# sahi_img_path = r\"/home/sanjeev_kumar/work/data/updated_data/combined_v1_demo/images/DJI_20211123144931_0006_W.JPG\"\n",
    "sahi_img_path =\"/home/sanjeev_kumar/work/trackouts/Images/DJI_20211123144914_0002_W.JPG\"\n",
    "\n",
    "# sahi_img_path= img_path_list[idx]\n",
    "result = get_sliced_prediction(\n",
    "    sahi_img_path,\n",
    "    sahi_model,\n",
    "    slice_height = 1024,\n",
    "    slice_width = 1024,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    "    postprocess_match_threshold=0.0  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e9b11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=\"test_t\"\n",
    "# print(c?v2.imread(sahi_img_path).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24f0bc0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mto_coco_annotations()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult_dict : \u001b[39m\u001b[38;5;124m\"\u001b[39m,result_dict)\n\u001b[0;32m      3\u001b[0m sahi_pred_list \u001b[38;5;241m=\u001b[39m extract_sahi_pred(result_dict,image_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result_dict = result.to_coco_annotations()\n",
    "print(\"result_dict : \",result_dict)\n",
    "sahi_pred_list = extract_sahi_pred(result_dict,image_size = (1024, 1024))\n",
    "print(\"sahi_pred_list : \",sahi_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1620ca69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sahi_pred_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sahi_box_img \u001b[38;5;241m=\u001b[39m sahi_draw_bounding_box(sahi_img_path,\u001b[43msahi_pred_list\u001b[49m)\n\u001b[0;32m      2\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m sahi_save_path \u001b[38;5;241m=\u001b[39m save_dir \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msahi_pred_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(idx)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sahi_pred_list' is not defined"
     ]
    }
   ],
   "source": [
    "sahi_box_img = sahi_draw_bounding_box(sahi_img_path,sahi_pred_list)\n",
    "save_dir =r\"/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/\"\n",
    "sahi_save_path = save_dir +\"sahi_pred_\"+str(idx)+\".JPG\"\n",
    "sahi_box_img.save(sahi_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f1b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0581ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img_path in enumerate(img_path_list):\n",
    "    result = get_sliced_prediction(\n",
    "    sahi_img_path,\n",
    "    sahi_model,\n",
    "    slice_height = 1024,\n",
    "    slice_width = 1024,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    "    postprocess_match_threshold=0.01  \n",
    "    )\n",
    "    result_dict = result.to_coco_annotations()\n",
    "    # print(\"result_dict : \",result_dict)\n",
    "    sahi_pred_list = extract_sahi_pred(result_dict,image_size = (1024, 1024))\n",
    "\n",
    "    sahi_box_img = sahi_draw_bounding_box(sahi_img_path,sahi_pred_list)\n",
    "    save_dir =r\"/home/sanjeev_kumar/work/usecase/trackouts_patch/pred_img/\"\n",
    "    sahi_save_path = save_dir +\"sahi_b_pred_\"+str(idx)+\".JPG\"\n",
    "    sahi_box_img.save(sahi_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c6d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f4de329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Image.open(sahi_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58931197",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m object_prediction_list \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mobject_prediction_list\n\u001b[0;32m      2\u001b[0m object_prediction_list\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "object_prediction_list = result.object_prediction_list\n",
    "object_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fa05c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mexport_visuals(export_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result.export_visuals(export_dir=\"demo_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "104618d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo_data/prediction_visual.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "Image(\"demo_data/prediction_visual.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251896",
   "metadata": {},
   "source": [
    "# sahi batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93574b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m betch_result \u001b[38;5;241m=\u001b[39m\u001b[43mpredict\u001b[49m(\n\u001b[0;32m      2\u001b[0m     detection_model\u001b[38;5;241m=\u001b[39msahi_model,\n\u001b[0;32m      3\u001b[0m     source\u001b[38;5;241m=\u001b[39mUNSEEN_DATA_PATH,\n\u001b[0;32m      4\u001b[0m     slice_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      5\u001b[0m     slice_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      6\u001b[0m     overlap_height_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      7\u001b[0m     overlap_width_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "betch_result =predict(\n",
    "    detection_model=sahi_model,\n",
    "    source=UNSEEN_DATA_PATH,\n",
    "    slice_height = 1024,\n",
    "    slice_width = 1024,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e457cc",
   "metadata": {},
   "source": [
    "# rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c371d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695e255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "080064e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_sliced_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sahi_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_sliced_prediction\u001b[49m(\n\u001b[0;32m      2\u001b[0m                 img,\n\u001b[0;32m      3\u001b[0m                 detection_model,\n\u001b[0;32m      4\u001b[0m                 slice_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      5\u001b[0m                 slice_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      6\u001b[0m                 overlap_height_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      7\u001b[0m                 overlap_width_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_sliced_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "sahi_model = get_sliced_prediction(\n",
    "                img,\n",
    "                detection_model,\n",
    "                slice_height = 1024,\n",
    "                slice_width = 1024,\n",
    "                overlap_height_ratio = 0.2,\n",
    "                overlap_width_ratio = 0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b13933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3dcb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba2d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27d2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7910e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sahi_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='torchvision',\n",
    "    model=model,\n",
    "    confidence_threshold=0.6,\n",
    "    image_size=1024,\n",
    "    device=device, # or \"cuda:0\"\n",
    "    load_at_init=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sahi_model(model,img,img_size=(1024,1024),batch=False):\n",
    "    if batch:\n",
    "        sahi_model = AutoDetectionModel.from_pretrained(\n",
    "                    model_type='torchvision',\n",
    "                    model=model,\n",
    "                    confidence_threshold=0.6,\n",
    "                    image_size=1024,\n",
    "                    device=device, # or \"cuda:0\"\n",
    "                    load_at_init=True,\n",
    "                    )\n",
    "    else:\n",
    "        sahi_model = get_sliced_prediction(\n",
    "                img,\n",
    "                detection_model,\n",
    "                slice_height = 1024,\n",
    "                slice_width = 1024,\n",
    "                overlap_height_ratio = 0.2,\n",
    "                overlap_width_ratio = 0.2,)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5192a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8bb40dd61e80645248f0d10eaba2fe17d2035115eccf99f367e6530105244766"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
